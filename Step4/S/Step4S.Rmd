---
title: "126 Data Project, Step 4"
date: "Sam Ream, Valeria Lopez, Skyler Yee"
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
# knit options
knitr::opts_chunk$set(echo = F,
                      results = 'markup',
                      fig.width = 4,
                      fig.height = 3,
                      fig.align = 'center',
                      message = F,
                      warning = F)

# packages
library(tidyverse)
library(faraway)
library(RSQLite)
library(skimr)
library(GGally)
library(tidymodels)
library(leaps)
library(glmnet)
library(corrr)
library(ggcorrplot)
library(FactoMineR)

library(factoextra)
```

```{r, echo=FALSE}
ball <- dbConnect(drv=RSQLite::SQLite(), dbname="../../data/database.sqlite")
```

```{r, echo =FALSE}
batting <- dbGetQuery(ball, "
                            SELECT
                            sum(ab) AS AT_BAT,
                            player_id,
                            sum(r) AS RUNS,
                            sum(hr) AS HOME_RUNS, 
                            sum(triple) AS TRIPLE,
                            sum(double) AS DOUBLE,
                            (sum(h) -  sum(hr) - sum(triple) - sum(double)) AS SINGLES,
                            sum(bb) AS WALKS,
                            sum(ibb) AS INT_WALKS,
                            sum(sb) AS STOLEN_BASES,
                            sum(hbp) AS HIT_BY_PITCH
                            
                            FROM
                            batting
                            where
                            year > 2000
                            group by
                            player_id
                            
                   ")
sumbat <- batting
batting <- subset(batting,batting[,1]>100)


for (x in 3:11) 
{
  batting[,x] <- batting[,x] / 1
}

set.seed(5)
batting <- sample_n(batting, 500, replace = FALSE)

players <- dbGetQuery(ball, "
                      
                      SELECT 
                      player_id,
                      (weight / POWER(height, 2)) *703 AS BMI,
                      bats as HAND
                      FROM
                      player
                      
                      ")




players <- subset(players, players[,2]>0)



for ( x in 1:17918)
{
  if(players[x,2] <= 18.5) {players[x,2] <- "U"} 
  else if(players[x,2] <= 24.9) {players[x,2] <- "H"}
  else if(players[x,2] <= 29.9) {players[x,2] <- "O"}
  else {players[x,2] <- "B"}
}

batting <-  merge(batting, players, by="player_id" )
```

```{r eval = FALSE}
#Final Model
stat_model <- lm(RUNS~HOME_RUNS + SINGLES + WALKS + STOLEN_BASES, data = batting)
summary(stat_model)
```

```{r}
fmod <- lm(RUNS ~ HOME_RUNS + TRIPLE + DOUBLE + SINGLES + WALKS + INT_WALKS + STOLEN_BASES + HIT_BY_PITCH, data=batting)
summary(fmod)
```

intro:

Using the "History of Baseball" data set, we analyzed how our predictors (singles, doubles, triples, home runs, walks, intentional walks, hit by pitches, stolen bases, BMI, and batting hand) affected the runs scores by individual players. We sampled player statistics randomly from games played between 2000-2015, which allowed us to get an accurate representation of the population of all players who played between 2000 and 2015. Using both Ridge Regression and LASSO, we shrunk the size of some predictors to obtain estimates with smaller variance for higher precision.

#Conclusion The data was what we had anticipated because the models had high R squared values, which indicates a large quantity of the variability can be explained by the regression. Using Ridge Regression, which aims to minimize SSE, we saw an R-squared value of 0.9914. Using LASSO regression, which shrinks the less important coefficients to zero, we got an R-squared value of 0.9936.

# Innovation Step: Principal Components Analysis

We chose Principal Components Analysis for our innovation because this method is used when there are a large number of predictors. The goal of this method is to replace our predictors with a smaller number of linear combinations of the predictors. We are essentially transforming our data into a lower-dimensional space while collating highly correlated variables together, allowing us to more easily understand and visualize our data. For example if we have $X_1, X_2,..., X_k$ predictors with k being large or at least $k\geq2$, we want to replace k with $k_0<k$ linear combinations of our predictors.

Let $\mathbf{X'} = (X_1, X_2,...X_k)$ and $\mathbf{u'}$ be a $p\times1$ vector of constants such that $\mathbf{u}_1'\mathbf{u}_1=1$. The first principal component will be the linear combination $Z_1=\mathbf{u'X}$ such that the variance of $Z_1 = \mathbf{u}_1'\text{Var}(\mathbf{X})\mathbf{u}_1$ is as large as possible to retain as much as the variation in the predictors as possible. If $\text{Var}(\mathbf{X})$ is known, then $\mathbf{u}_i$'s are the eigenvectors that corresponds to the $k_0$ largest eigenvalues of $\text{Var}(\mathbf{X})$. If $\text{Var}(\mathbf{X})$ is unknown, like in our case, we replace the variance matrix with the sample covariance matrix.

First we normalize the data by dividing by the sample standard deviation, compute the PCA using the sample correlation matrix, and assess the cumulative proportion of each principal component to see which principal components explain the most of the total variance. Then look at the loading matrix to see how these principal components relate to each column of our data.



### Calculations

```{r}
# Removing unneccessary columns
head(batting)
numerical_data <- batting[,4:11]
head(numerical_data)

# Data normalization
data_normalized <- scale(numerical_data)
head(data_normalized)

# PCA computation
data.pca <- princomp(data_normalized)
summary(data.pca)

data.pca$comp.1

# Loading matrix
data.pca$loadings[, 1:2] # come back: using right components?
```

### Visualizations

```{r}
# Scree plot - importance of each component 
fviz_eig(data.pca, addlabels=TRUE)

# Biplot of the attributes - visualize similiarities between samples and see impact of each attribute on components
fviz_pca_var(data.pca, col.var="black")

# Contribution of each variable - how much each variable is represented in a given component (utilizing the square of cosine) 
# High value means good representation
fviz_cos2(data.pca, choice="var", axes=1:2)

# Biplot combined with cos2 - attributes with similar cos2 scores will have similar colors
fviz_pca_var(data.pca, col.var = "cos2",
            gradient.cols = c("black", "orange", "green"),
            repel = TRUE)

# Doubles contributes the most; home runs, walks, singles, triples, and stolen bases also contribute highly
# home runs, walks, doubles, singles positively correlated
# triples, singles, stolen bases negatively correlated
# all have similar magnitude (distance from origin)

# choose home runs, walks, doubles 
```

### Analysis
By looking at our calculations and visualizations, we can see that 

### Problems that Could Arise

If the data is not a random sample from the population, then the variables will be measured on some arbitrary scale that depends on the sampling design since the sample standard deviations used to standardize the variables will not align with the population. Our sample is a random sample from our population, so we do not run into this issue.
