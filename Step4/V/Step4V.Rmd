---
title: "126 Data Project, Step 4"
date: "Sam Ream, Valeria Lopez, Skyler Yee"
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
# knit options
knitr::opts_chunk$set(echo = F,
                      results = 'markup',
                      fig.width = 4,
                      fig.height = 3,
                      fig.align = 'center',
                      message = F,
                      warning = F)

# packages
library(tidyverse)
library(faraway)
library(RSQLite)
library(skimr)
library(GGally)
library(tidymodels)
library(leaps)
library(glmnet)
```

```{r, echo=FALSE}
ball <- dbConnect(drv=RSQLite::SQLite(), dbname="../../data/database.sqlite")
```

```{r, echo =FALSE}
batting <- dbGetQuery(ball, "
                            SELECT
                            sum(ab) AS AT_BAT,
                            player_id,
                            sum(r) AS RUNS,
                            sum(hr) AS HOME_RUNS, 
                            sum(triple) AS TRIPLE,
                            sum(double) AS DOUBLE,
                            (sum(h) -  sum(hr) - sum(triple) - sum(double)) AS SINGLES,
                            sum(bb) AS WALKS,
                            sum(ibb) AS INT_WALKS,
                            sum(sb) AS STOLEN_BASES,
                            sum(hbp) AS HIT_BY_PITCH
                            
                            FROM
                            batting
                            where
                            year > 2000
                            group by
                            player_id
                            
                   ")
sumbat <- batting
batting <- subset(batting,batting[,1]>100)


for (x in 3:11) 
{
  batting[,x] <- batting[,x] / 1
}

set.seed(5)
batting <- sample_n(batting, 500, replace = FALSE)

players <- dbGetQuery(ball, "
                      
                      SELECT 
                      player_id,
                      (weight / POWER(height, 2)) *703 AS BMI,
                      bats as HAND
                      FROM
                      player
                      
                      ")




players <- subset(players, players[,2]>0)



for ( x in 1:17918)
{
  if(players[x,2] <= 18.5) {players[x,2] <- "U"} 
  else if(players[x,2] <= 24.9) {players[x,2] <- "H"}
  else if(players[x,2] <= 29.9) {players[x,2] <- "O"}
  else {players[x,2] <- "B"}
}

batting <-  merge(batting, players, by="player_id" )
```

# Introduction

Using the "History of Baseball" data set we analyzed how our predictors (singles, doubles, triples, home runs, walk, intentional walks, hit by pitches, stolen bases, BMI, and batting hand) affected the runs scores by individual players. Our population was was sampled randomly from the years 2000-2015, which allowed us to get an accurate representation of the population and represent the changing strategies of baseball. 

```{r eval = FALSE}
#Final Model
stat_model <- lm(RUNS~HOME_RUNS + SINGLES + WALKS + STOLEN_BASES, data = batting)
summary(stat_model)
```

# Ridge Regression

```{r}
y <- batting$RUNS
x <- data.matrix(batting[, c('AT_BAT', 'HOME_RUNS', 'SINGLES', 'WALKS', 'STOLEN_BASES', 'WALKS', 'INT_WALKS', 'STOLEN_BASES', 'HIT_BY_PITCH')])
```

## Fit the Ridge Regression Model

```{r}
ridge_model <- glmnet(x, y, alpha = 0)
summary(ridge_model)
```

## Choose an Optimal Value for Lambda

```{r}
cv_model_ridge <- cv.glmnet(x, y, alpha = 0)

# find optimal lambda value that minimizes test MSE
best_lambda_r <- cv_model_ridge$lambda.min
best_lambda_r

# produce plot of test MSE by lambda value
plot(cv_model_ridge)
```

The lambda value that minimizes the test MSE is 24.53741.

## Analyze Final Model

```{r}
#find coefficients of best model
best_model <- glmnet(x, y, alpha=0, lamba = best_lamba_r)

#coef(best_model_ridge)
```

```{r}
# produce ridge trace plot
plot(ridge_model, xvar = 'lambda')
```

```{r}
#calculate R-squared
y_predicted <- predict(ridge_model, s = best_lambda_r, newx =x)

#SST and SSE
sst <- sum((y-mean(y))^2)
sse <- sum((y_predicted - y)^2)

rsq <- 1-(sse/sst)
rsq
```

The R-Squared is 0.9914, so the best model explains 99.14% of the variation in the response values.

# LASSO

```{r}
y <- batting$RUNS
x <- data.matrix(batting[, c('AT_BAT', 'HOME_RUNS', 'SINGLES', 'WALKS', 'STOLEN_BASES', 'WALKS', 'INT_WALKS', 'STOLEN_BASES', 'HIT_BY_PITCH')])
```

## Fit the Lasso Regression Model

```{r}
model <- glmnet(x, y, alpha = 1)
summary(model)
```

## Choose an Optimal Value for Lambda

```{r}
cv_model <- cv.glmnet(x, y, alpha = 1)

# find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda

# produce plot of test MSE by lambda value
plot(cv_model)
```

The lambda value that minimizes the test MSE is 0.364.

## Analyze Final Model

```{r}
#find coefficients of best model
best_model <- glmnet(x, y, alpha=1, lamba = best_lamba)

#coef(best_model)
```

```{r}
# produce ridge trace plot
plot(model, xvar = 'lambda')
```

```{r}
#calculate R-squared
y_predicted <- predict(model, s = best_lambda, newx =x)

#SST and SSE
sst <- sum((y-mean(y))^2)
sse <- sum((y_predicted - y)^2)

rsq <- 1-(sse/sst)
rsq
```

The R-Squared is 0.9936, so the best model explains 99.36% of the variation in the response values.

#Conclusion The data was what we had anticipated because they had high R squared values, so a lot of the variability could by explained by the regression. Using Ridge regression, which aims to minimize SSE, the R-squared was 0.9914. Using LASSO regression, which shrinks the less important coefficients to zero, the R-squared was 0.9936.
