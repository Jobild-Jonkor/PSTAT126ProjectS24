---
title: "126 Data Project, Step 4"
date: "Sam Ream, Valeria Lopez, Skyler Yee"
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
# knit options
knitr::opts_chunk$set(echo = F,
                      results = 'markup',
                      fig.width = 4,
                      fig.height = 3,
                      fig.align = 'center',
                      message = F,
                      warning = F)

# packages
library(tidyverse)
library(faraway)
library(RSQLite)
library(skimr)
library(GGally)
library(tidymodels)
library(leaps)
library(glmnet)
library(gridExtra)
```
```{r, echo=FALSE}
ball <- dbConnect(drv=RSQLite::SQLite(), dbname="../../data/database.sqlite")
```
```{r, echo =FALSE}
batting <- dbGetQuery(ball, "
                            SELECT
                            sum(ab) AS AT_BAT,
                            player_id,
                            sum(r) AS RUNS,
                            sum(hr) AS HOME_RUNS, 
                            sum(triple) AS TRIPLE,
                            sum(double) AS DOUBLE,
                            (sum(h) -  sum(hr) - sum(triple) - sum(double)) AS SINGLES,
                            sum(bb) AS WALKS,
                            sum(ibb) AS INT_WALKS,
                            sum(sb) AS STOLEN_BASES,
                            sum(hbp) AS HIT_BY_PITCH
                            
                            FROM
                            batting
                            where
                            year > 2000
                            group by
                            player_id
                            
                   ")
sumbat <- batting
batting <- subset(batting,batting[,1]>100)


for (x in 3:11) 
{
  batting[,x] <- batting[,x] / 1
}

set.seed(5)
batting <- sample_n(batting, 500, replace = FALSE)

players <- dbGetQuery(ball, "
                      
                      SELECT 
                      player_id,
                      (weight / POWER(height, 2)) *703 AS BMI,
                      bats as HAND
                      FROM
                      player
                      
                      ")




players <- subset(players, players[,2]>0)



for ( x in 1:17918)
{
  if(players[x,2] <= 18.5) {players[x,2] <- "U"} 
  else if(players[x,2] <= 24.9) {players[x,2] <- "H"}
  else if(players[x,2] <= 29.9) {players[x,2] <- "O"}
  else {players[x,2] <- "B"}
}

batting <-  merge(batting, players, by="player_id" )
```



```{r echo = FALSE}
#Final Model
stat_model <- lm(RUNS~HOME_RUNS + SINGLES + WALKS + STOLEN_BASES, data = batting)
summary(stat_model)
```

# Ridge Regression
```{r}
y <- batting$RUNS
x <- data.matrix(batting[, c('AT_BAT', 'HOME_RUNS', 'SINGLES', 'WALKS', 'STOLEN_BASES', 'WALKS', 'INT_WALKS', 'STOLEN_BASES', 'HIT_BY_PITCH')])
```

## Fit the Ridge Regression Model 
```{r}
ridge_model <- glmnet(x, y, alpha = 0)
summary(ridge_model)
```
## Choose an Optimal Value for Lambda 
```{r}
cv_model_ridge <- cv.glmnet(x, y, alpha = 0)

# find optimal lambda value that minimizes test MSE
best_lambda_r <- cv_model_ridge$lambda.min
best_lambda_r

# produce plot of test MSE by lambda value
plot(cv_model_ridge)
```
The lambda value that minimizes the test MSE is 24.53741.  

## Analyze Final Model
```{r}
#find coefficients of best model
best_model <- glmnet(x, y, alpha=0, lamba = best_lamba_r)

#coef(best_model_ridge)
```


```{r}
# produce ridge trace plot
plot(ridge_model, xvar = 'lambda')
```









```{r}
#calculate R-squared
y_predicted <- predict(ridge_model, s = best_lambda_r, newx =x)

#SST and SSE
sst <- sum((y-mean(y))^2)
sse <- sum((y_predicted - y)^2)

rsq <- 1-(sse/sst)
rsq
```
The R-Squared is 0.9914, so the best model explains 99.14% of the variation in the response values. 

# LASSO
```{r}
y <- batting$RUNS
x <- data.matrix(batting[, c('AT_BAT', 'HOME_RUNS', 'SINGLES', 'WALKS', 'STOLEN_BASES', 'WALKS', 'INT_WALKS', 'STOLEN_BASES', 'HIT_BY_PITCH')])
```

## Fit the Lasso Regression Model 
```{r}
model <- glmnet(x, y, alpha = 1)
summary(model)
```

## Choose an Optimal Value for Lambda 
```{r}
cv_model <- cv.glmnet(x, y, alpha = 1)

# find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda



# produce plot of test MSE by lambda value
plot(cv_model)
```
The lambda value that minimizes the test MSE is 0.364. 

## Analyze Final Model
```{r}
#find coefficients of best model
best_model <- glmnet(x, y, alpha=1, lamba = best_lamba)

#coef(best_model)
```


```{r}
# produce ridge trace plot
plot(model, xvar = 'lambda')
```


```{r}
#calculate R-squared
y_predicted <- predict(model, s = best_lambda, newx =x)

#SST and SSE
sst <- sum((y-mean(y))^2)
sse <- sum((y_predicted - y)^2)

rsq <- 1-(sse/sst)
rsq
```
The R-Squared is 0.9936, so the best model explains 99.36% of the variation in the response values. 



































```{r}




cv_model$lambda.min




```























































```{r}
ridge_model_y_predictions <- data.frame(Prediction = 1:500, Actual= 1:500)
ridge_model_y_predictions[,1] <- predict(ridge_model, s = best_lambda_r, newx = x)
ridge_model_y_predictions[,2] <- batting$RUNS

lasso_model_y_predictions <- data.frame(Prediction = 1:500, Actual = 1:500)
lasso_model_y_predictions[,1] <- predict(cv_model, s = best_lambda_r, newx = x)
lasso_model_y_predictions[,2] <- batting$RUNS


stat_model_y_predictions <- data.frame(Prediction = 1:500, Actual = 1:500)
stat_model_y_predictions[,1] <- predict(stat_model, newx = x)
stat_model_y_predictions[,2] <- batting$RUNS

```
```{r, fig.width=7,fig.height=3}
ggplot() + 
  
  geom_point(aes(x=Prediction, y=Actual, color="MLR"), stat_model_y_predictions,size = 0.5) + 
  geom_smooth(aes(x=Prediction, y=Actual, color="MLR"), stat_model_y_predictions, method = lm)+
  
  geom_point(aes(x=Prediction, y=Actual, color="Ridge Model"), ridge_model_y_predictions, size =0.5)+
  geom_smooth(aes(x=Prediction, y=Actual, color="Ridge Model"), ridge_model_y_predictions, method = lm)+
  
  geom_point(aes(x=Prediction, y=Actual, color="Lasso Model"), lasso_model_y_predictions, size =0.5 )+
  geom_smooth(aes(x=Prediction, y=Actual, color="Lasso Model"), lasso_model_y_predictions, method = lm)+
  labs(color = "Model")




```
