---
title: "126 Data Project, Step 2"
date: "Sam Ream, Valeria Lopez, Skyler Yee"
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
# knit options
knitr::opts_chunk$set(echo = F,
                      results = 'markup',
                      fig.width = 4,
                      fig.height = 3,
                      fig.align = 'center',
                      message = F,
                      warning = F)

# packages
library(tidyverse)
library(faraway)
library(RSQLite)
library(skimr)
library(GGally)
library(tidymodels)
library(leaps)
```
```{r, echo=FALSE}
ball <- dbConnect(drv=RSQLite::SQLite(), dbname="../../data/database.sqlite")
```
```{r, echo =FALSE}
batting <- dbGetQuery(ball, "
                            SELECT
                            sum(ab) AS AT_BAT,
                            player_id,
                            sum(r) AS RUNS,
                            sum(hr) AS HOME_RUNS, 
                            sum(triple) AS TRIPLE,
                            sum(double) AS DOUBLE,
                            (sum(h) -  sum(hr) - sum(triple) - sum(double)) AS SINGLES,
                            sum(bb) AS WALKS,
                            sum(ibb) AS INT_WALKS,
                            sum(sb) AS STOLEN_BASES,
                            sum(hbp) AS HIT_BY_PITCH
                            
                            FROM
                            batting
                            where
                            year > 2000
                            group by
                            player_id
                            
                   ")
sumbat <- batting
batting <- subset(batting,batting[,1]>100)


for (x in 3:11) 
{
  batting[,x] <- batting[,x] / 1
}

set.seed(5)
batting <- sample_n(batting, 500, replace = FALSE)

players <- dbGetQuery(ball, "
                      
                      SELECT 
                      player_id,
                      (weight / POWER(height, 2)) *703 AS BMI,
                      bats as HAND
                      FROM
                      player
                      
                      ")




players <- subset(players, players[,2]>0)



for ( x in 1:17918)
{
  if(players[x,2] <= 18.5) {players[x,2] <- "U"} 
  else if(players[x,2] <= 24.9) {players[x,2] <- "H"}
  else if(players[x,2] <= 29.9) {players[x,2] <- "O"}
  else {players[x,2] <- "B"}
}

batting <-  merge(batting, players, by="player_id" )
```

```{r, eval = FALSE, fig.width=10, fig.height=10}
ggpairs(batting, columns = 3:13)
```

```{r, eval= FALSE, fig.width=10, fig.height=10}
ggpairs(batting, columns = c("INT_WALKS","AT_BAT", "WALKS","STOLEN_BASES", "HIT_BY_PITCH"))
```

```{r, eval= FALSE, fig.width=10, fig.height=10}
ggpairs(batting, columns = c("INT_WALKS","SINGLES", "TRIPLE","STOLEN_BASES", "HOME_RUNS"))
```






# Introduction



# Analysis of Variables 

When investigating the predictors, we noted it did not appear that we needed to use any transformation to make the data more linear. Additionaly, it is clear that some predictors were highly correlated. An example of this is Singles and Doubles which have a correlation of 0.95. As a consequence of this, we elected to use only one of these two variables in our hand-made model and did the same with other predictors with similar levels of correlation.

We decided to not include interaction variables because different values of our categorical variables (BMI and handedness) do not drastically affect the response. We also felt it was not necessary for any of our non-categorical variables as there are no interactions that we believe to be interesting.

# Computational Models

For our computational models, we used the predictors: Total Intentional walks, Singles, Triples, Stolen Bases, and Home_Runs obtained in a career. We selected these predictors because of their low correlation in addition to their interesting relation to obtained Runs. To help prevent over-correlation, we also elected to create a reduced model using only predictors related to hitting the ball and compared the two to see if we could use a smaller model.

### Model 1 - Full Model ( $\Omega$ )

$\mathbb{E}[Y]=\text{Intercept } + \text{Intentional Walks }+\text{Singles }+\text{Triples }+\text{Stolen Bases }+\text{Home Runs}+\epsilon$

### Model 2 - Reduced Model ( $\omega$ )

$\mathbb{E}[Y]=\text{Intercept } + \text{Singles }+\text{Triples }+\text{Home Runs}+\epsilon$

### Comparison:

$H_0: \beta \in \omega :\text{The Reduced Model is sufficient}$

$H_\alpha: \beta \in \Omega \text{\\} \omega \in w :\text{The reduced Model is not sufficient}$


```{r}
model_1 <- lm(RUNS~INT_WALKS + SINGLES + TRIPLE + STOLEN_BASES + HOME_RUNS, data = batting)
model_2 <- lm(RUNS~SINGLES + TRIPLE + HOME_RUNS, data = batting)
anova(model_1, model_2)
```

### Conclusion

As we rejected $H_0$ in favor for $H_\alpha$, we can determine that the reduced model does not model the data well enough to justify the reduction in predictors. As such, we decided to use model 1, the full model, as our computational model.





# Statistical Models

We used a stepwise search to create the best model for our data. For a size of 4 predictors the variables home runs, singles, walks, and stolen bases create a well fit model.

```{r eval = FALSE}
stat_model <- regsubsets(RUNS~HOME_RUNS + TRIPLE + DOUBLE + SINGLES + WALKS + INT_WALKS + STOLEN_BASES + HIT_BY_PITCH, data = batting,
                  method = 'seqrep',
                  nbest = 1,
                  nvmax = 4)
summary(stat_model)
```



### Residual Plot and Summary table

```{r}
stat_model <- lm(RUNS~HOME_RUNS + SINGLES + WALKS + STOLEN_BASES, data = batting)
res <- resid(stat_model)
plot(fitted(stat_model), res, ylab = 'Residuals', xlab = 'Fitted Model')
abline(0,0)
```
```{r}
summary(lm(RUNS~HOME_RUNS + SINGLES + WALKS + STOLEN_BASES, data = batting))
```

# Final model selection
Between the two models we created, the statistical model and computational model, we selected the Statistical model. The reason behind this selection is that the statistical model has a larger $R_{adj}^2$ value and we want to explain as much of the variance as possible in our model.


# Analysis of the Final Model:


- Interpret $\beta_i$s and intercept. Are they significant? (SAM)

- Report R^2 and adj R^2/interpret/discuss (SAM)






```{r, fig.width=10, fig.height=4}

studentize <- function(resid)
  {
    resid*sqrt((500 - 5 - 1)/(500 - 5 - resid^2))
  }

par(mfrow = c(1, 2))
plot(augment(stat_model)$.hat)








exStdResid <- augment(stat_model)$.std.resid
exStdResid <- as.data.frame(exStdResid)

exStdResid[,2] <- exStdResid[,1]
exStdResid[,1] <- 1:500
colnames(exStdResid) <- (c("Index", "External Studentized Residuals"))
exStdResid[,2] <- studentize(exStdResid[,2])


plot(exStdResid)
abline(3,0)
abline(-3,0)
plot(augment(stat_model)$.cooksd)

```



















- Complete analysis of residuals and influence points. Use plots/consider refitting the data with points that have large leverage and residuals (KOSYS)

- interpret the model in a way that makes sense. Why do you think some variables dropped out? (KOSYS)

- Give CIs for a mean predicted value and the PIs of a future predicted value for at lease one combination of X's (VALERIA)

# Summary

- Summarize (VALERIA)




